<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"halloju.github.io","root":"/","scheme":"Muse","version":"8.0.0-rc.5","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false};
  </script>

  <meta name="description" content="Transformer 為什麼需要一層 pointwiseFFN ? 一開始看 “Attention is all you need” 時，心裡隱隱有這個疑問。我以為對於 contexual representation 而言 “Attention is all I need”，self-attention 的機制可以讓模型直接獲得前後文本資訊，且經由訓練學習每個 token 要花多少注意力在哪些">
<meta property="og:type" content="article">
<meta property="og:title" content="pointwiseFFN">
<meta property="og:url" content="https://halloju.github.io/2021/04/pointwiseFFN/index.html">
<meta property="og:site_name" content="Juver">
<meta property="og:description" content="Transformer 為什麼需要一層 pointwiseFFN ? 一開始看 “Attention is all you need” 時，心裡隱隱有這個疑問。我以為對於 contexual representation 而言 “Attention is all I need”，self-attention 的機制可以讓模型直接獲得前後文本資訊，且經由訓練學習每個 token 要花多少注意力在哪些">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.imgur.com/n6S0lwX.png">
<meta property="og:image" content="https://i.imgur.com/KdsPPZO.png">
<meta property="og:image" content="https://i.imgur.com/FERrSDL.png">
<meta property="article:published_time" content="2021-04-13T03:58:14.000Z">
<meta property="article:modified_time" content="2021-04-14T04:34:02.768Z">
<meta property="article:author" content="Juju">
<meta property="article:tag" content="transformers, NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.imgur.com/n6S0lwX.png">

<link rel="canonical" href="https://halloju.github.io/2021/04/pointwiseFFN/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>pointwiseFFN | Juver</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Juver</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer-%E7%82%BA%E4%BB%80%E9%BA%BC%E9%9C%80%E8%A6%81%E4%B8%80%E5%B1%A4-pointwiseFFN"><span class="nav-number">1.</span> <span class="nav-text">Transformer 為什麼需要一層 pointwiseFFN ?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-Understanding-Parsing-Composition"><span class="nav-number">2.</span> <span class="nav-text">Language Understanding: Parsing&#x2F;Composition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#composition%EF%BC%88%E5%89%8D%E5%BE%8C%E6%96%87%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">composition（前後文）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parsing-%E5%8F%A5%E5%AD%90%E7%B5%90%E6%A7%8B"><span class="nav-number">2.2.</span> <span class="nav-text">parsing (句子結構)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E7%A2%BA%E7%9A%84%E8%A7%A3%E6%9E%90"><span class="nav-number">2.3.</span> <span class="nav-text">正確的解析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT-%E7%95%B6%E4%B8%AD%E7%9A%84-parsing-composition"><span class="nav-number">3.</span> <span class="nav-text">BERT 當中的 parsing&#x2F;composition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%9E%E7%B7%9A%E6%80%A7%E7%9A%84%E7%B5%84%E5%90%88"><span class="nav-number">3.1.</span> <span class="nav-text">非線性的組合</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">4.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Juju</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://halloju.github.io/2021/04/pointwiseFFN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Juju">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Juver">
    </span>

    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          pointwiseFFN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-13 11:58:14" itemprop="dateCreated datePublished" datetime="2021-04-13T11:58:14+08:00">2021-04-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-04-14 12:34:02" itemprop="dateModified" datetime="2021-04-14T12:34:02+08:00">2021-04-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h3 id="Transformer-為什麼需要一層-pointwiseFFN">Transformer 為什麼需要一層 pointwiseFFN ?</h3>
<p>一開始看 “Attention is all you need” 時，心裡隱隱有這個疑問。我以為對於 contexual representation 而言 “Attention is all I need”，self-attention 的機制可以讓模型直接獲得前後文本資訊，且經由訓練學習每個 token 要花多少注意力在哪些其他 token 上 。那 pointwiseFFN 究竟是要學習什麼呢？ 後來上網搜尋發現鄉民也有同樣的疑問， 以下就統整我目前所發現的解釋跟我一些想法。<br>
<img src="https://i.imgur.com/n6S0lwX.png" alt=""></p>
<a id="more"></a>
<h3 id="Language-Understanding-Parsing-Composition">Language Understanding: Parsing/Composition</h3>
<p>文字意義是經由上下文本賦予，這是在學 word2vec 時很重要的概念，我也一直用這個想法來理解 transformer。但後來我發現 context 的概念應該要再拆成 <strong>parsing/composition</strong> 兩個元素，再用這兩個元素來理解 transformer 的架構。</p>
<h4 id="composition（前後文）">composition（前後文）</h4>
<p>顧名思義就是將兩個或以上的字合在一起看，而相同的字跟不同的字合在一起看可能就代表不同的意思，例如，(“river”, “bank”), (“bank”, “account”)，兩個的 bank 是不同的意思。</p>
<h4 id="parsing-句子結構">parsing (句子結構)</h4>
<p>之前上 stanford cs224N 時其實有一堂課專門在講這個概念，但一心追求 BERT 的我就沒放在心上。其實 parsing 要做的事就是拆解句子的階層架構，經典的例子如，</p>
<blockquote>
<p>Bart watched a squirrel with binoculars</p>
</blockquote>
<p>可以將句子拆解成以下的架構， 可以看到 <code>with binocular</code> 就是用來修飾 <code>watched a squirrel</code>，他們是在樹狀圖的同一階層下。<br>
<img src="https://i.imgur.com/KdsPPZO.png" alt=""></p>
<p>但同時也可以其畫成以下的結構，可以發現這時 <code>with binocular</code> 跟 <code>a squirrel</code> 。 是在同一層，這時 <code>with binocular</code> 變成修飾 <code>a squirrel</code> 。<br>
<img src="https://i.imgur.com/FERrSDL.png" alt=""></p>
<p>下面的拆解法其實文法是正確的，但意義上其實是不合理的。<br>
我們還可以舉出很多例子，又例如，</p>
<blockquote>
<p>Scientists count whales from space</p>
</blockquote>
<p>這個 <code>from space</code>  同樣有模糊空間，因為你不知道這是來修飾 <code>count whales</code> 還是 <code>whales</code> 本人。</p>
<p>這時要正確的 parsing 其實就需要正確的 composition ，也就是告訴 parsing ，（“a squirrel”,“with binocular”）放在一起不合理。</p>
<h4 id="正確的解析">正確的解析</h4>
<p>完整的句子解析同時需要 composition 跟 parsing，而他們又是互相依存的，也就是好的 parsing 需要 composition，好的 composition 需要 parsing 。因為 composition 會根據 parsing 的結果進行組合，例如，(“a”, “squirrel”), (“watched”, “a squirrel”)。而正確的 composition 可以幫助 parsing，例如，(“watched a squirrel”,“with binocular”) v.s. (“a squirrel”,“with binocular”)，如果模型可以給前項的 watch 有意義的向量，後項 squirrel 的向量很小，就可以幫助 parsing 進行判斷。</p>
<h3 id="BERT-當中的-parsing-composition">BERT 當中的 parsing/composition</h3>
<p>了解了 composition/parsing，其實不難發現，attention 的機制就如同 parsing，克服句子很長的情形，attend 到同一階層的其他 token。而這時 pointwiseFFN 就扮演 composition 的角色，將 attention 當中全部相加的 embedding，透過 autoencoding 的方式解析。</p>
<h4 id="非線性的組合">非線性的組合</h4>
<p>假設 self-attention 層真的把 <code>with binocular</code> attend 到 <code>a squirrel</code> ，這時只是單純的將所有 token 的 embedding 進行線性組合可能不夠，畢竟， <code>watched</code> 跟 <code>a squirrel</code> 還有 <code>with</code> 跟 <code>binoculars</code> 不在同一階層，可能需要非線性的組合。</p>
<h3 id="Reference">Reference</h3>
<p>上面那篇 reddit 有留言分享一篇很有內容的文章，<a href="%5Bhttps://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db%5D(https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db)">Understanding BERT Transformer: Attention isn’t all you need</a>。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/transformers-NLP/" rel="tag"># transformers, NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/Nomadland/" rel="prev" title="《游牧人生》影評：所有的相遇都是久別重逢">
      <i class="fa fa-chevron-left"></i> 《游牧人生》影評：所有的相遇都是久別重逢
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Juju</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  















  

  

</body>
</html>
